{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "Deep learning is probably the most exciting and potential filled aspect of machine learning. Deep learning has been around for quite awhile but it has taken until recently for it to truly take off. This is because computers were not where they needed to be to handle the computational complexity needed for deep learning until recently. I will likely do a seperate series of these notebooks focusing just on deep learning because there is alot to be said and it is really is the future of machine learning. The idea of deep learning, at a high level, is to mimic the human brain in computers. The human brain is the most intricate and effective learning tools in the known universe. Through millions of years of evolution, natural selection has created an incredible learning machine. Being able to mimic the human brain in computer would revolutionize a computers ability to learn. At a very high level, deep learning algorithm consist of an input layer, hidden layers, and an output layer. The input layer is where we would input our data, the hidden layers is where all of the learning would occur, and the output layer would where our result would be. This is modeled in the picture below:\n",
    "\n",
    "![deep_learning](deep_learning.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will be a crash course overview of deep learning focusing on two very important kinds of deep learning algorithms: the artificial nueral network and the convolution neural network.\n",
    "\n",
    "# The Artificial Neural Network\n",
    "\n",
    "The artifical neural network is the most common kind of deep learning. Before we can talk about the neural network, we need to talk about the *neuron*.  The neuron is the building block of the human brain and will also be the building block of our neural network. A picture of a single neuron is shown below.\n",
    "![neuron](neuron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neurons in and of themselves are pretty much useless. It is when neurons are connected that they become so powerful. The dendrite of a neuron connect to the axons of other neurons, allow neurons to create powerful networks where signals can be sent through the network with ease. When modeling this in our computer, we will simplify the physiology of the neuron. As shown in the diagram of the neural network. Neurons will simply have inputs that come from other neurons and then an output that also connects to the other neurons (except in the case of the first and last layer of neurons). Neurons will send the signal on (in neuroscience, this is refered to as a neuron \"firing\") based on an *activation function*. We will discuss activation functions in much more detail, but basically they are the function that determines whether or not a neuron will fire. Now that we a basic idea of the neuron, we have to talk about how the neurons are connected together. We will call the lines that connect neurons *synapses*. The synapses will all have a weight associated with them that determine how strong or important the signal being passed is. The weights of the neurons are where the learning actually occurs. As the network learns, the weights on the synapses are adjusted such that important signals are giving higher weights and less important signals get lower weights. Basically, in each neuron, we take the sum of all signals coming in multiplied by their weights and that is the value that a neuron works with. That value will be passed into the activation function which will decide if that signal is strong enough to cause the neuron to fire. Put more formally, if we have *n* different neurons sending a signal to our neuron (we will denote the output of a neuron as *x*) and the synapse the output travels along before arriving at our neuron has weight deonted *w*, then the value that will be passed into our activation function is $\\sum_{i=1}^n x_iw_i$. \n",
    "\n",
    "Now that we have the value being passed into our neuron, we have to decide what to do with it. As mentioned before, this is where the *activation function* comes into play. Four different activation functions are shown below: \n",
    "![threshold](threshold.jpg)\n",
    "![sigmoid](sigmoid.jpg)\n",
    "![rectifier](rectifier.jpg)\n",
    "![tanh](tanh.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of these, the rectifier function is one of the most important, but which activation function you choose is entirely dependent on your data. For example, if our variables are binary, the we would have to choose either the threshold function or a modified sigmoid function. A very common combination to use is the rectifier function for the hidden layers and then the sigmoid function for the output layer so that our final output is a probability. \n",
    "\n",
    "So we already mentioned that the neural network learns by modifying the weights of the synapses, but we need to now discuss how that weights are modified. We start the same way that we would with many of the machine models we have seen so far. Let's assume that we have some independent variables and we want to use them to predict some dependent variable. We first take the independent variables and feed them through our network. Note that we start with all weights set to a default value (let's say 1). Our network will them output a value $\\hat{y}$ which we will compare to the actual value $y$. We will use a *cost function* to quantify the difference. The cost function for a single observation might be $C = \\frac{1}{2}(\\hat{y}-y)^2$. This is just one cost function but we could use others. Our goal is to minimize the cost function, so we will use the value output by the cost function and feed it back into our network. Using this information, the weights will be updated. We can then run our experiment again on the same data. The value of our cost function should now be lower, but it is likely not as low as it can be. We will get a new value for the cost function and use it to again tweak our weights. We continually do this until the weights converge and our cost function is minimized. We just discussed how to do this with a single observation, but extending it to multiple is easy. Run the network on all observations and compute a total cost function as $C = \\sum\\frac{1}{2}(\\hat{y}-y)^2$. Re-run on all observations until the cost function converges.\n",
    "\n",
    "Let us now speak about mathematically how the weights are actually adjusted. This is done through a process called *backpropogation*, where the value of the cost function is fed back through the network in order to adjust the weights. Now a brute force way to compute these weights would be to choose a ton of different values for the weights and see what works best. But when we have large networks, the amount of synapses increases exponentially, thus the amount of weights we have to figure out is huge. Trying to brute force a huge amount of variables simultaneously becomes computationally gargantuan. In fact, using only a decently large network, the worlds fastest computer would take longer to compute all the combinations for these synapses ten the universe has existed, so we need a smarter way. The method we will use to speed things up will be called *gradient descent*. Basically, we choose some starting values for our weights. We feed these into the cost function and get a value, but we also look at the slope of our cost function (the derivate) at that value. If our slope is negative, that means increasing the values would get a smaller value for the cost function, so we will increase our weights. A positive slope means decreasing our weights would give us a smaller value for the cost function, so we will decrease our weights. We continue until our slope is 0, meaning we have reach a local minimum in our cost function. The actual process is a bit more complex but this is in general what we are trying to do. The following visual should help sure up this concept.\n",
    "\n",
    "![gd](gradient_descent.jpg)\n",
    "\n",
    "Now the main problem with simple gradient descent is that it requires the cost function to be convex. Convex here means that it has one global minimum and all roads points to it, so to speak. This often does not happen. More complex cost functions, or even simple cost functions in higher dimensions have many ups and down. So our algorithm may find a minimum and stop, but that minimum is a local minimum and we could have done better had we found the global minimum. The solution here is *stochastic gradient descent*, which does not require a convex cost function. The main difference in stochastic gradient descent is when the weight is updated. In regular gradient descent, we run all of our observations, get an overall value for the cost function, and then update the weights before running all rows again. This is also known as batch gradient descent. Stochastic gradient descent does each row at a time. It takes the first observations and keeps running it throught the network until the cost function converges, then it moves on to the second row and does the same. It turns out that because stochastic gradient descent does one row at a time, the fluctuations are much higher, meaning it is much more likely to land in the global minimum. For reasons that involve how computer memory works, stochastic gradient descent is also faster. The big con is that stochastic gradient descent is stochastic (meaning random) meaning if you ran the same stochastic gradient descent twice on the same data, you might not get the same weights. There is a mix of the two called mini-batch gradient descent that does a few observations at a time.\n",
    "\n",
    "The last concept to mention before we jump into the application is *backpropogation*. Now backpropogation has some heavy mathematics behind it, but at a high level, it means that we know what part of the error each layer is responsible for thus we can update all weights at the same time. With all of this said, let us quickly recap before we jump in the algorithm for stochastic gradient descent.\n",
    "1. Randomly initialize weights with small numbers that are close to but different from 0.\n",
    "2. Input the first observation of your data set into the input layer, each feature is one input node.\n",
    "3. Forward propogate the data through the network. At each layer, neurons are activated based on the weights. Propogate until we get a predicted value $\\hat{y}$. \n",
    "4. Compare the predicted result to the actual result $y$. Use this to get an error determined by the cost function. \n",
    "5. Backpropogate this information through the network and update all weights accordingly. \n",
    "6. Repeat steps 3 through 5 after each observation for stochastic gradient descent or after a batch of observations for regular gradient descent.\n",
    "7. After all data has passed through, that is one epoch. Do more epochs until the weights converge. \n",
    "\n",
    "Now that we have this description of the algorithm, we are going to do a business case with it. The dataset for this example is a sample from a ficticious bank. This bank has been noticing that an unusually high number of people leaving the bank. The bank wants to figure out what people are at highest risk for leaving, which is when they turned to you. 6 months ago, the bank selected 10000 random customers from their huge customer base. At that time, they collected all of the information they had on those customers. Just recently, the bank went back and recorded whether or not those customers have left the bank in the past 6 months. They now want you to create an artificial neural network of this data to predict what customers are at the highest risk of leaving. Before we can build our model, we need to install some libraries to help us out. The first library is Theano which is a numerical computation library that can run computations on both CPU's and GPU's. Another similiar library is Tensorflow. These libraries are both great for building neural networks from scratch, but we would like to build networks faster. Enter the Keras library which is built on the Theano and Tensorflow libraries and allows us to build neural networks quickly and easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
