{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "Natural language processing (NLP) is a series of algorithms that help us deal with text and human speech. If you've ever used Siri, Alexa, Google Home, or any of the plethora of smart devices with voice assistant, then you have seen the implications of natural language processing. NLP is a very broad term consisting of many different processes and algorithms, and it also has many applications. As mentioned, one use is human text to speech. Another popular use, and the use that we will look at, is sentiment analysis. The idea behind seniment analysis is to take a variety of text input and try to figure out if the text is positive or negative. For example, we will be looking at a bunch of restaurant reviews. These reviews also have a binary variable that indicate whether the review was favorable or not (1 if the review is favorable and 0 if it is not). Using this data, we want to create a machine learning model that can take a new review and decide whether or not it is positive or negative. As you may have noticed, this is very similiar to the classification section from earlier. In fact, the algorithms we used in that section will work here as well! The main difference when doing NLP is that the preprocessing step is more involved. We have to take the raw text and format it in a way that we can pass it into the classification model.\n",
    "\n",
    "# Preprocessing\n",
    "\n",
    "The most crucial step of NLP is preprocessing the data such that we can pass it into the algorithms. This step is text and algorithm agnostic. This mean that though we are using it for reviews, this step can be easily modified to handle books, conversations or really any other text data. Once we do this step, we can input the results into many different classification algorithms. This is why we say that this step is text and algorithm agnostic. As mentioned, our data set is a tab seperated file where each line contains a review and then whether or not the review was positive or negative. Most algorithms for NLP expect a *bag of words* as input. The bag of words model simply views text as the words it contains and how often those words appear. For example, the bag of words interpretation of the sentence \"The cow jumped over the moon\" is:\n",
    "* \"the\": 2\n",
    "* \"cow\": 1\n",
    "* \"jumped\": 1\n",
    "* \"over\": 1\n",
    "* \"moon\": 1\n",
    "\n",
    "The bag or words model ignores many things that most humans view as important, like word order in grammar. It is an extremely simplified version of text, but often works quite well. This isn't quite the end of the story though because we also need to take into account words that our text *doesn't* have. In other words, when we build our feature set (remember features are what we actually pass into the machine learning model), we are going to have a column for every relevant word we have ever seen. Then for each review, we fill in the values for how many times each of our features appears in the review. This may raise some red flags since most reviews are very short and only contain a few words. This is true and because of this, each row in our set will contain mostly 0's. To solidify this, let us return to our sentence above, \"the cow jumps over the moon\". Our features for this text might be every english word, so when we fill in our features we will put a 1 in the columns for the words \"cow\", \"jumped\", \"over\" and \"moon, a 2 in the \"the\" column, and a 0 in the column for every other english word. So this will be an extremely scarce feature matrix. In order to help us limit the number of columns we will have to use, we need to remove every word from our reviews that doesn't help us towards our end goal of classifying a review as positive or negative. With our end goal of bag of words in mind, let us go over the steps to get there. For each review, we do the following:\n",
    "\n",
    "1. Remove anything that is not the alphabet. Numbers, puncuation, and symbols don't really tell us anything about the review's sentiment, so take them out. Always replace these with a space so we don't create hybrid words.\n",
    "2. Make the entire review lowercase for consistancy.\n",
    "3. Split up the sentence on spaces to create a list of individual words.\n",
    "4. Remove any 'stopwords'. Stopwords are words that are so common, that they don't distinguish individual text at such, such as 'a', 'the', 'my', ect.\n",
    "5. *Stem* the words. Stemming is the process of taking just the root from a verb. For example, \"love\", \"loves\", \"loved\", and \"loving\" tell us essentially the same thing, but they are treated as different in our current setup. Stemming takes all of these different tenses and just replaces them with \"love\".\n",
    "6. Rejoin our list of words to a sentence. This is because there are already functions created to turn text into a bag of words (also called tokenization), but these functions need sentences and not lists of words.\n",
    "\n",
    "Given that this is our process, let us implement it in python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('data/Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)\n",
    "\n",
    "# Cleaning the texts\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "corpus = []\n",
    "for i in range(0, 1000):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list of reviews in the variable *corpus*. Now we need to convert this corpus into a bag of words model. So first we need to create our feature matrix. The columns of this feature matrix will be every unique word in our corpus. Then, each review will get one row in our matrix and the row will contain how many times each word in our columns appear for that given review. Remember that the most of our row will contain just 0's. We will use built in function from the \"sklearn\" library. In order to keep our matrix from getting too big, we will input a max features parameters into our Tokenization function. This will only choose the most frequently used words in our corpus. For example, if we set max features to 1500, our model will only consider the 1500 most frequent words across our corpus. You can also restrict the size in other ways, but we will not worry too much about this now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = dataset.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data split up into dependent and independent variables, we can use the exact same code from the classification section to make our model. We will use the Naive Bayes algorithm, but any of the classification algorithms we learned can be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[55 42]\n",
      " [12 91]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Student\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "# Fitting Naive Bayes to the Training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our confusion matrix shows that the algorithm only did an ok job of classifying. There are a number of things we could do to improve the results, including using a different algorithm and tweaking some of our parameters, but this was just an overview.\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "This was just a quick overview of the very broad domain that is natural language processing. One easy way to improve models is to change our feature set. In this example, we just used word count as our feature, but there are other features we could of used. The most popular of these is *term frequency-inverse document frequency* (tf-idf). This metric still uses word count, but it weights this value by how often the word appears across the corpus. So in our example, the word \"food\" might show up many times in a review, but it will also show up in almost every review. This means that it is less helpful of a word, so the *tf-idf* would be lower. Again, there are many other things to learn about NLP, but this should serve as a good overview and the code for the preprocessing step should be helpful in getting started for any NLP problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
