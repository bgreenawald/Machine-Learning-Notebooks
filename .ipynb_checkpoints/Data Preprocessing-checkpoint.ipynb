{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of the data preprocessing process is to import our necessary libraries.  Numpy and pandas are two primary python libraries used in data science. The big draw of numpy is that it provides a data structure that is comparable to the dataframe in R. Pandas is another very powerful analytics library. The final library is matplotlib, which as the name implies is a plotting python library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we actually need to import our data. Since our data is stored as a CSV (comma seperated value) file, we will use the read_csv function provdided by pandas, which takes only our file name as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Country   Age   Salary Purchased\n",
      "0   France  44.0  72000.0        No\n",
      "1    Spain  27.0  48000.0       Yes\n",
      "2  Germany  30.0  54000.0        No\n",
      "3    Spain  38.0  61000.0        No\n",
      "4  Germany  40.0      NaN       Yes\n",
      "5   France  35.0  58000.0       Yes\n",
      "6    Spain   NaN  52000.0        No\n",
      "7   France  48.0  79000.0       Yes\n",
      "8  Germany  50.0  83000.0        No\n",
      "9   France  37.0  67000.0       Yes\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/Data.csv\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our full set of data, but we need to split it up into our features and our response. This is like the independent and dependent variables that one will have encountered in any introductory science course. Our features will be used to build the model, which attempts to predict the response. Country, Ade, and Salary are the features, which will attempt to predict Purchased, which, as in many machine learning models, is binary (\"yes\" or \"no\"). The *iloc* function is part of pandas and is used as an indexing tool for csv datasets. The general syntax is the index of the rows and then the columns, and take the form 'x:y'. The way indexing works in python is that it will take all data starting at the value given before the colon up to but not including the value given after the colon. For example, '1:3' would take the first observation and the second observation, and would stop and not inlude the third observation. One other small syntactic python caveat is that we can do negative indexing, or indexing starting at the end of a list. We do this by employing negative indexes. The index '-1' represents the **last** element in a list, '-2' the **second to last** observation, and so on. So turing out attention to the first line of code nelow, the first ':' in the *iloc* function tells the function that we want all rows. For the $X$ variable, the ':-1' as the second argument to the *iloc* function means that we want all columns except the last one. For the $y$ variable, we just enter a '3' for the second argument to the *iloc* function as we want only the third column (for those coming from R or other languages, remember that python, like many programming languages, uses zero-based indexing. This means that the first element in many data structures is indexed by a '0')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X =  [['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 nan]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' nan 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]] \n",
      "\n",
      "y =  ['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, 3].values\n",
    "print(\"X = \", X, '\\n')\n",
    "print(\"y = \", y, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we must take care of missing data. Many modern datasets will lack some data and it is very important to deal with it in an intelligent way. One intuition might be to remove a particular observation if it has any missing data. This is generally not considered a great practice because as datasets get larger, we expect many observations to have some sort of missing data. It seems quite wasteful to remove a perfectly good observation just because one of it features lacks data. For a sparse data set, such a practice could remove the majority of our data set. Because of this, we explore different methods. One logical solution, and the one used here, will be to replace any missing values with the mean of the column. This allows us to get all of the rest of the information from a given row without skewing the data all that much. Now this of course only works numeric columned values (you can't take the mean of a country). There are solutions to filling in missing categorical values, but they will not be explored at this point. Moving forward with our \"replace with the average idea\", we will turn to an object from the sklearn library, another extremely powerful data analytics python library, called *Imputer*. After importing the necessary method, we create an instance of the *Imputer* object and tell it that we want to use the mean as the method. Next, we fit the object on the columns that have missing data (using the *fit* method), which are the second and third columns (which since we used zero-based indexing means the columns indexed 1,2). Remember that to index these columns, we used '1:3' which means start at the column indexed 1 and end at but do not included the column indexed 3. After we have fit the object, we replace the columns in $X$ with the columns if $X$ after calling the transform method, which is done on the second to last line. In the last line, we print the results and can see that our missing values have indeed been replaced with the average of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)\n",
    "imputer = imputer.fit(X[:, 1:3])\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we must encode any categorical variables we might have. When builidng machine learning models, we are training a mathematical model on data, and strings (like 'Country' in this data set) have no meaning as mathematical objects. That is, until we encode them. We will not go over the details of how these object are encoded here, but the general methodology is to created $n-1$ disjoint dummy variables, which can take on values 0 and 1, for your objects. For example, we might have two dummy variables named *spain* and *france*. If *spain* is 1, then the given observation is from Spain. If *france* is 1, the given observation is from France. Since they are disjoint, both *spain* and *france* cannot be 1 at the same time. You might be wondering how Germany is represented since we only have variables for *spain* and *france*. A given observation is from Germany if both *spain* and *france* are 0. Again, these details are secondary as we are more concerned with how to actually do it in code. The process is a bit syntax heavy it will be helpful to spend some time looking over the code, but the process is largely the same as above. Import and create the necessary objects from sklearn, fit those objects on the data that needs encoding (in this case only the first column, or column with index 0 from $X$), then set that column equal to its transformed version. We also do the same this for $y$ since 'Yes' and 'No' also have no meaning in models without encoding. The output does not look pretty since python is outputing high predicision scientific notated versions of our data, but we can see that the encoding has indeed occured. Python does encoding a little differently as it has a binary variable for each of our different kinds of observations, but the idea is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X =  [[  1.00000000e+00   0.00000000e+00   0.00000000e+00   4.40000000e+01\n",
      "    7.20000000e+04]\n",
      " [  0.00000000e+00   0.00000000e+00   1.00000000e+00   2.70000000e+01\n",
      "    4.80000000e+04]\n",
      " [  0.00000000e+00   1.00000000e+00   0.00000000e+00   3.00000000e+01\n",
      "    5.40000000e+04]\n",
      " [  0.00000000e+00   0.00000000e+00   1.00000000e+00   3.80000000e+01\n",
      "    6.10000000e+04]\n",
      " [  0.00000000e+00   1.00000000e+00   0.00000000e+00   4.00000000e+01\n",
      "    6.37777778e+04]\n",
      " [  1.00000000e+00   0.00000000e+00   0.00000000e+00   3.50000000e+01\n",
      "    5.80000000e+04]\n",
      " [  0.00000000e+00   0.00000000e+00   1.00000000e+00   3.87777778e+01\n",
      "    5.20000000e+04]\n",
      " [  1.00000000e+00   0.00000000e+00   0.00000000e+00   4.80000000e+01\n",
      "    7.90000000e+04]\n",
      " [  0.00000000e+00   1.00000000e+00   0.00000000e+00   5.00000000e+01\n",
      "    8.30000000e+04]\n",
      " [  1.00000000e+00   0.00000000e+00   0.00000000e+00   3.70000000e+01\n",
      "    6.70000000e+04]]\n",
      "y =  [0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X = LabelEncoder()\n",
    "X[:, 0] = labelencoder_X.fit_transform(X[:, 0])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "print('X = ', X)\n",
    "\n",
    "labelencoder_y = LabelEncoder()\n",
    "y = labelencoder_y.fit_transform(y)\n",
    "print('y = ', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The penultimate step in data preprocessing it to split our data into a training and a testing sets. What this means is that we want to split up our data into two disjoint sets of observations. The 'train' set of this data is what the model is going to learn on. Once the model has learned on the train data, we will use the 'test' set to see how well the model is able to preform. We do this by feeding the features of an observation into our model. The model will output a prediction based on the features and we then compare the prediction given by the model to the actual value of the response for that observation. It is absolutely critical that there is no overlap between these sets. To understand why, consider this scenario. A teacher always gives out a practice exam before the actual exam and wants to know how performance on the practice exam relates to performance on the real exam (the practice exam is our 'train' set and the real exam is our 'test' set). If there is overlap between the questions on the practice exam and the real exam, then the performance on the real exam is going to be biased. If a student studies well on the practice and then sees some of the exact same questions on the real exam, then they are obviously going to get the questions they have already seen right on the final, thus the teachers results are biased and not fully representative of how well the students learned. The obvious next question is \"how much of our data should be train and how much should be test?\". The answer is: it varies. Maybe not the most concrete answer, but true nonetheless. For a very large dataset, it is possible to get away with a 50-50 split, but the general starting point is 80-20 train-test. It is then up to the person creating the model to tinker with these values to achieve the best result. sklearn provides an extremely easy way to split up into train test, shown below. From the output, we see that $X$ has indeed been split up into two disjoint sets with a ration of 80-20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train =  [[  0.00000000e+00   1.00000000e+00   0.00000000e+00   4.00000000e+01\n",
      "    6.37777778e+04]\n",
      " [  1.00000000e+00   0.00000000e+00   0.00000000e+00   3.70000000e+01\n",
      "    6.70000000e+04]\n",
      " [  0.00000000e+00   0.00000000e+00   1.00000000e+00   2.70000000e+01\n",
      "    4.80000000e+04]\n",
      " [  0.00000000e+00   0.00000000e+00   1.00000000e+00   3.87777778e+01\n",
      "    5.20000000e+04]\n",
      " [  1.00000000e+00   0.00000000e+00   0.00000000e+00   4.80000000e+01\n",
      "    7.90000000e+04]\n",
      " [  0.00000000e+00   0.00000000e+00   1.00000000e+00   3.80000000e+01\n",
      "    6.10000000e+04]\n",
      " [  1.00000000e+00   0.00000000e+00   0.00000000e+00   4.40000000e+01\n",
      "    7.20000000e+04]\n",
      " [  1.00000000e+00   0.00000000e+00   0.00000000e+00   3.50000000e+01\n",
      "    5.80000000e+04]] \n",
      "\n",
      "X test =  [[  0.00000000e+00   1.00000000e+00   0.00000000e+00   3.00000000e+01\n",
      "    5.40000000e+04]\n",
      " [  0.00000000e+00   1.00000000e+00   0.00000000e+00   5.00000000e+01\n",
      "    8.30000000e+04]] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Student\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "print(\"X train = \", X_train, \"\\n\")\n",
    "print(\"X test = \", X_test, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A final step for many machine learning models is called feature scaling. The intuition behind feature scaling can get a bit mathematical so not much time will be spent trying to understand it, but it is good practice. The idea behind feature scaling is that if variables are on extremely different scales, then the model may become biased. Take for example our Age and Salary features. Age ranges from 27 to 50 while salary 48000 to 83000. If we were to create a model out of this, the term for Salary would dominate the term for Age because the values for Salary are just so much larger and have so much larger of a scale than Age. The remedy is to scale all values to have roughly the same range. An example scaling procedure might be $x_{scale}=\\frac{x_{obs} - \\mu_{x}}{\\sigma_{x}}$ where $\\mu_{x}$ is the mean of the $x$ values and $\\sigma_{x}$ is the standard deviation of $x$. For those who have taken statistics courses, this should look very similar to the z-value or t-value in hypothesis testing, which often fall between -4 and 4. There are many other scaling methods and which one is used is largely a concern, though one can tinker with different ones to see which works best. Here, we will just let sklearn do it for us. The procedure is again similar to what we have already seen so looking through the code below should be sufficient to understand what is happening. From the output, we can see that our values for Age and Salary have indeed been scaled to have a common range. Note that there really isn't a need to scale binary variables though it won't hurt if one does it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train =  [[-1.          2.64575131 -0.77459667  0.26306757  0.12381479]\n",
      " [ 1.         -0.37796447 -0.77459667 -0.25350148  0.46175632]\n",
      " [-1.         -0.37796447  1.29099445 -1.97539832 -1.53093341]\n",
      " [-1.         -0.37796447  1.29099445  0.05261351 -1.11141978]\n",
      " [ 1.         -0.37796447 -0.77459667  1.64058505  1.7202972 ]\n",
      " [-1.         -0.37796447  1.29099445 -0.0813118  -0.16751412]\n",
      " [ 1.         -0.37796447 -0.77459667  0.95182631  0.98614835]\n",
      " [ 1.         -0.37796447 -0.77459667 -0.59788085 -0.48214934]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "print(\"X train = \", X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now seen a general overview of how to preprocess data for machine learning models. It is critical to undergo these steps as needed before starting any sort of machine learning model to ensure the accuracy and integrity of the model created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
